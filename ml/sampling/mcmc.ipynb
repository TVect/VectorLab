{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = stats.multivariate_normal(mean=[0, 0], cov=np.array([[10, 6], [6, 10]]))\n",
    "samples = dis.rvs(1000)\n",
    "# plt.scatter(samples[:, 0], samples[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using gibbs sampler\n",
    "\n",
    "def gibbs_sampler_of_gaussian(mean, cov, burnin=0, num=100):\n",
    "    ''' 2 dims '''\n",
    "    # calculate conditional distribution p(x_i | x_{-i})\n",
    "    x = [mean[0], mean[1]]\n",
    "    precision = cov.I\n",
    "    samples = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        # p(x0 | x1)\n",
    "        x0_var = 1 / precision[0, 0]\n",
    "        x0_mean = mean[0] - x0_var * precision[0, 1] * (x[1] - mean[1])\n",
    "        x0_sample = np.random.normal(x0_mean, x0_var)\n",
    "        x[0] = x0_sample\n",
    "        # p(x1 | x0)\n",
    "        x1_var = 1 / precision[1, 1]\n",
    "        x1_mean = mean[1] - x1_var * precision[0, 1] * (x[0] - mean[0])\n",
    "        x1_sample = np.random.normal(x1_mean, x1_var)\n",
    "        x[1] = x1_sample\n",
    "        if i > burnin:\n",
    "            samples.append([x[0], x[1]])\n",
    "        if len(samples) >= num:\n",
    "            break\n",
    "        i += 1\n",
    "        \n",
    "    return np.array(samples)\n",
    "\n",
    "samples = gibbs_sampler_of_gaussian(mean=[0, 0], cov=np.matrix([[10, 6], [6, 10]]), burnin=1000, num=1000)\n",
    "# plt.scatter(samples[:, 0], samples[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMC 参考资料\n",
    "\n",
    "- [**blog & visualizations**: Hamiltonian Monte Carlo explained](http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html)\n",
    "\n",
    "- [**blog & code**: Hamiltonian Monte Carlo from scratch](https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "from autograd import numpy as auto_np\n",
    "\n",
    "def hamiltonian_monte_carlo(n_samples, negative_log_prob, initial_position, path_len=1, step_size=0.5):\n",
    "    \"\"\"Run Hamiltonian Monte Carlo sampling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples to return\n",
    "    negative_log_prob : callable\n",
    "        The negative log probability to sample from\n",
    "    initial_position : np.array\n",
    "        A place to start sampling from.\n",
    "    path_len : float\n",
    "        How long each integration path is. Smaller is faster and more correlated.\n",
    "    step_size : float\n",
    "        How long each integration step is. Smaller is slower and more accurate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        Array of length `n_samples`.\n",
    "    \"\"\"\n",
    "    # autograd magic\n",
    "    dVdq = grad(negative_log_prob)\n",
    "\n",
    "    # collect all our samples in a list\n",
    "    samples = [initial_position]\n",
    "\n",
    "    # Keep a single object for momentum resampling\n",
    "    momentum = stats.norm(0, 1)\n",
    "\n",
    "    # If initial_position is a 10d vector and n_samples is 100, we want\n",
    "    # 100 x 10 momentum draws. We can do this in one call to momentum.rvs, and\n",
    "    # iterate over rows\n",
    "    size = (n_samples,) + initial_position.shape[:1]\n",
    "    for p0 in momentum.rvs(size=size):\n",
    "        # Integrate over our path to get a new position and momentum\n",
    "        q_new, p_new = leapfrog(\n",
    "            samples[-1],\n",
    "            p0,\n",
    "            dVdq,\n",
    "            path_len=path_len,\n",
    "            step_size=step_size,\n",
    "        )\n",
    "\n",
    "        # Check Metropolis acceptance criterion\n",
    "        start_log_p = negative_log_prob(samples[-1]) - auto_np.sum(momentum.logpdf(p0))\n",
    "        new_log_p = negative_log_prob(q_new) - auto_np.sum(momentum.logpdf(p_new))\n",
    "        if auto_np.log(auto_np.random.rand()) < start_log_p - new_log_p:\n",
    "            samples.append(q_new)\n",
    "        else:\n",
    "            samples.append(auto_np.copy(samples[-1]))\n",
    "\n",
    "    return auto_np.array(samples[1:])\n",
    "\n",
    "\n",
    "def leapfrog(q, p, dVdq, path_len, step_size):\n",
    "    \"\"\"Leapfrog integrator for Hamiltonian Monte Carlo.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q : np.floatX\n",
    "        Initial position\n",
    "    p : np.floatX\n",
    "        Initial momentum\n",
    "    dVdq : callable\n",
    "        Gradient of the velocity\n",
    "    path_len : float\n",
    "        How long to integrate for\n",
    "    step_size : float\n",
    "        How long each integration step should be\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q, p : np.floatX, np.floatX\n",
    "        New position and momentum\n",
    "    \"\"\"\n",
    "    q, p = auto_np.copy(q), auto_np.copy(p)\n",
    "\n",
    "    p -= step_size * dVdq(q) / 2  # half step\n",
    "    for _ in range(int(path_len / step_size) - 1):\n",
    "        q += step_size * p  # whole step\n",
    "        p -= step_size * dVdq(q)  # whole step\n",
    "    q += step_size * p  # whole step\n",
    "    p -= step_size * dVdq(q) / 2  # half step\n",
    "\n",
    "    # momentum flip at end\n",
    "    return q, -p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using HMC\n",
    "\n",
    "def neg_log_prob(x, mean, cov):\n",
    "    return  auto_np.dot(auto_np.dot(x-mean, cov.I.getA()), x-mean)\n",
    "\n",
    "mean = auto_np.array([0.0, 0.0])\n",
    "cov = auto_np.matrix([[10.0, 6.0], [6.0, 10.0]])\n",
    "\n",
    "samples = hamiltonian_monte_carlo(n_samples=1000, \n",
    "                                  negative_log_prob=lambda x: neg_log_prob(x, mean, cov), \n",
    "                                  initial_position=auto_np.array([0.0, 0.0]), \n",
    "                                  path_len=1.0, \n",
    "                                  step_size=0.5)\n",
    "\n",
    "# plt.scatter(samples[:, 0], samples[:, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
