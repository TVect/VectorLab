{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手工构造样本不均衡的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: Counter({4: 6038, 3: 3004, 2: 1179, 1: 1164, 0: 615})\n",
      "testing set: Counter({4: 1973, 3: 951, 1: 444, 2: 430, 0: 202})\n",
      "valid set: Counter({4: 1934, 3: 1048, 2: 415, 1: 411, 0: 192})\n"
     ]
    }
   ],
   "source": [
    "weights=[0.05, 0.1, 0.1, 0.25, 0.5]\n",
    "\n",
    "X, y = make_classification(n_samples=20000,        # 样本个数\n",
    "                           n_features=20,          # 特征个数\n",
    "                           n_informative=5,        # 有效特征个数\n",
    "                           n_redundant=2,          # 冗余特征个数（有效特征的随机组合）\n",
    "                           n_repeated=0,           # 重复特征个数（有效特征和冗余特征的随机组合）\n",
    "                           n_classes=5,            # 样本类别\n",
    "                           n_clusters_per_class=2, # 簇的个数\n",
    "                           weights=weights,\n",
    "                           class_sep=1.5,\n",
    "#                            class_sep=0.3,\n",
    "                           random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,random_state = 33,test_size = 0.2)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train,y_train,random_state = 33,test_size = 0.25)\n",
    "\n",
    "print(f\"training set: {Counter(y_train)}\")\n",
    "print(f\"testing set: {Counter(y_valid)}\")\n",
    "print(f\"valid set: {Counter(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试一：不考虑样本不均衡问题，直接训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianxiaoliang/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.21      0.33       615\n",
      "           1       0.55      0.30      0.39      1164\n",
      "           2       0.84      0.81      0.83      1179\n",
      "           3       0.59      0.50      0.54      3004\n",
      "           4       0.72      0.89      0.80      6038\n",
      "\n",
      "    accuracy                           0.69     12000\n",
      "   macro avg       0.69      0.54      0.58     12000\n",
      "weighted avg       0.68      0.69      0.67     12000\n",
      "\n",
      "[[ 129   40  122   79  245]\n",
      " [   2  347    3  401  411]\n",
      " [  24    0  953   26  176]\n",
      " [   1  183   32 1513 1275]\n",
      " [  22   56   18  561 5381]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.23      0.36       192\n",
      "           1       0.59      0.30      0.40       411\n",
      "           2       0.88      0.80      0.84       415\n",
      "           3       0.60      0.51      0.55      1048\n",
      "           4       0.70      0.89      0.78      1934\n",
      "\n",
      "    accuracy                           0.69      4000\n",
      "   macro avg       0.71      0.55      0.58      4000\n",
      "weighted avg       0.68      0.69      0.67      4000\n",
      "\n",
      "[[  45   13   29   17   88]\n",
      " [   0  123    1  135  152]\n",
      " [  11    0  330   10   64]\n",
      " [   0   52   12  533  451]\n",
      " [   3   21    2  187 1721]]\n"
     ]
    }
   ],
   "source": [
    "# 训练过程中不做 balance\n",
    "model = LogisticRegression(class_weight=None, solver='lbfgs').fit(x_train,y_train)\n",
    "y_train_pred = model.predict(x_train)\n",
    "print(classification_report(y_pred=y_train_pred, y_true=y_train))\n",
    "print(confusion_matrix(y_pred=y_train_pred, y_true=y_train))\n",
    "\n",
    "y_test_pred = model.predict(x_test)\n",
    "print(classification_report(y_pred=y_test_pred, y_true=y_test))\n",
    "print(confusion_matrix(y_pred=y_test_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试二: 在训练过程中做 balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianxiaoliang/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.56      0.50       615\n",
      "           1       0.40      0.66      0.50      1164\n",
      "           2       0.68      0.82      0.75      1179\n",
      "           3       0.54      0.53      0.54      3004\n",
      "           4       0.85      0.69      0.76      6038\n",
      "\n",
      "    accuracy                           0.66     12000\n",
      "   macro avg       0.58      0.65      0.61     12000\n",
      "weighted avg       0.69      0.66      0.67     12000\n",
      "\n",
      "[[ 347   50  190   16   12]\n",
      " [   3  770    4  287  100]\n",
      " [ 146    5  971   16   41]\n",
      " [  17  651  161 1590  585]\n",
      " [ 262  457   96 1027 4196]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.56      0.50       192\n",
      "           1       0.43      0.66      0.52       411\n",
      "           2       0.69      0.82      0.75       415\n",
      "           3       0.56      0.55      0.55      1048\n",
      "           4       0.84      0.71      0.77      1934\n",
      "\n",
      "    accuracy                           0.66      4000\n",
      "   macro avg       0.60      0.66      0.62      4000\n",
      "weighted avg       0.69      0.66      0.67      4000\n",
      "\n",
      "[[ 108   15   59    1    9]\n",
      " [   0  272    0  106   33]\n",
      " [  57    1  340    5   12]\n",
      " [   1  206   64  572  205]\n",
      " [  71  138   28  332 1365]]\n"
     ]
    }
   ],
   "source": [
    "# 训练过程中做 balance\n",
    "model = LogisticRegression(class_weight='balanced', solver='lbfgs').fit(x_train,y_train)\n",
    "y_train_pred = model.predict(x_train)\n",
    "print(classification_report(y_pred=y_train_pred, y_true=y_train))\n",
    "print(confusion_matrix(y_pred=y_train_pred, y_true=y_train))\n",
    "\n",
    "y_test_pred = model.predict(x_test)\n",
    "print(classification_report(y_pred=y_test_pred, y_true=y_test))\n",
    "print(confusion_matrix(y_pred=y_test_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方案三: 阈值移动（threshold-moving）\n",
    "\n",
    "即：训练阶段不做 balance, 在预测阶段 后处理调整了 threshold\n",
    "\n",
    "参考：[分类任务中数据类别不平衡问题](https://blog.csdn.net/kuaizi_sophia/article/details/84894363)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianxiaoliang/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.65      0.44       615\n",
      "           1       0.38      0.72      0.50      1164\n",
      "           2       0.67      0.80      0.73      1179\n",
      "           3       0.53      0.52      0.53      3004\n",
      "           4       0.88      0.62      0.72      6038\n",
      "\n",
      "    accuracy                           0.62     12000\n",
      "   macro avg       0.56      0.66      0.58     12000\n",
      "weighted avg       0.70      0.62      0.64     12000\n",
      "\n",
      "[[ 401   47  145    9   13]\n",
      " [   4  836   24  233   67]\n",
      " [ 199    6  941    7   26]\n",
      " [  54  806  172 1566  406]\n",
      " [ 569  506  123 1120 3720]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.63      0.42       192\n",
      "           1       0.41      0.71      0.52       411\n",
      "           2       0.67      0.80      0.73       415\n",
      "           3       0.55      0.54      0.54      1048\n",
      "           4       0.86      0.62      0.72      1934\n",
      "\n",
      "    accuracy                           0.62      4000\n",
      "   macro avg       0.56      0.66      0.59      4000\n",
      "weighted avg       0.69      0.62      0.64      4000\n",
      "\n",
      "[[ 121   13   48    2    8]\n",
      " [   1  291    4   87   28]\n",
      " [  71    1  330    4    9]\n",
      " [  12  256   69  562  149]\n",
      " [ 174  153   38  373 1196]]\n"
     ]
    }
   ],
   "source": [
    "# 训练阶段不做 balance, 在预测阶段 后处理调整了 threshold\n",
    "\n",
    "model = LogisticRegression(class_weight=None, solver='lbfgs').fit(x_train,y_train)\n",
    "pred = model.predict_proba(x_train)\n",
    "pred = pred / weights\n",
    "y_train_pred = np.argmax(pred, axis=1)\n",
    "\n",
    "print(classification_report(y_pred=y_train_pred, y_true=y_train))\n",
    "print(confusion_matrix(y_pred=y_train_pred, y_true=y_train))\n",
    "\n",
    "pred = model.predict_proba(x_test)\n",
    "pred = pred / weights\n",
    "y_test_pred = np.argmax(pred, axis=1)\n",
    "print(classification_report(y_pred=y_test_pred, y_true=y_test))\n",
    "print(confusion_matrix(y_pred=y_test_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方案三：温度后调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用启发式的方法做温度调整\n",
    "\n",
    "使用启发式的方法做温度调整时，损失函数并不直接与优化目标（预测样本类别占比和真实样本类别占比相近）一致，最终的结果比启发式方法的结果要差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianxiaoliang/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= iteration: 9 diff: 0.02150000000000002, temperature: [1.24203019 1.35518875 1.03340533 0.93733938 0.61518134]\n",
      "========= iteration: 19 diff: 0.009750000000000002, temperature: [1.41180022 1.41695954 1.04957558 0.86211325 0.55423574]\n",
      "========= iteration: 29 diff: 0.005500000000000005, temperature: [1.51451286 1.38918277 1.06671883 0.83350806 0.53632827]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.48      0.52       192\n",
      "           1       0.46      0.52      0.49       411\n",
      "           2       0.80      0.81      0.80       415\n",
      "           3       0.59      0.55      0.57      1048\n",
      "           4       0.79      0.81      0.80      1934\n",
      "\n",
      "    accuracy                           0.69      4000\n",
      "   macro avg       0.64      0.63      0.64      4000\n",
      "weighted avg       0.69      0.69      0.69      4000\n",
      "\n",
      "[[  92   20   49    6   25]\n",
      " [   0  215    1  114   81]\n",
      " [  43    3  336    8   25]\n",
      " [   0  161   27  576  284]\n",
      " [  25   67    9  273 1560]]\n"
     ]
    }
   ],
   "source": [
    "# 训练过程中不做 balance，在预测阶段，为每个类别调整 temperature\n",
    "\n",
    "x_valid_new = x_valid\n",
    "y_valid_new = y_valid\n",
    "x_test_new = x_test\n",
    "y_test_new = y_test\n",
    "\n",
    "model = LogisticRegression(class_weight=None, solver='lbfgs').fit(x_train,y_train)\n",
    "\n",
    "raw_valid_probs = model.predict_proba(x_valid_new)\n",
    "raw_test_probs = model.predict_proba(x_test_new)\n",
    "\n",
    "temperature = [1.0] * 5\n",
    "y_valid_counter = Counter(y_valid_new)\n",
    "target_distrib = [y_valid_counter[key]/len(y_valid_new) for key in sorted(Counter(y_valid_new).keys())]\n",
    "\n",
    "\n",
    "def auto_tuning_temperature_heuristic(model, x_val, y_val, max_diff=0.005, max_iter=100):\n",
    "    ''' 调整温度，使得 model 的预测输出分布 和 y_val 中分布接近\n",
    "    @params model: 模型，需要有方法 predict_proba\n",
    "    '''\n",
    "    label_counter = Counter(y_val)\n",
    "    temperature = [1.0 for _ in range(len(label_counter))]\n",
    "    y_prob = model.predict_proba(x_val)\n",
    "    y_true_distrib = [label_counter[key]/len(y_valid) for key in sorted(label_counter.keys())]\n",
    "    \n",
    "    learning_rate = 1e-1\n",
    "    learning_rate = 0.9\n",
    "    for t in range(100):\n",
    "        y_prob_tuned = softmax(np.log(y_prob) / temperature, axis=1)\n",
    "        y_pred_tuned = np.argmax(y_prob_tuned, axis=1)\n",
    "        \n",
    "        tuned_label_counter = Counter(y_pred_tuned)\n",
    "        y_pred_tuned_distrib = [tuned_label_counter[key]/len(y_valid) \n",
    "                                for key in sorted(tuned_label_counter.keys())]\n",
    "        diff = np.array(y_true_distrib) - np.array(y_pred_tuned_distrib)\n",
    "        if t % 10 == 9:\n",
    "            print(f\"========= iteration: {t} diff: {np.max(np.abs(diff))}, temperature: {temperature}\")\n",
    "        if max(abs(diff)) < max_diff:\n",
    "            break\n",
    "        # temperature = temperature * (np.exp(diff))\n",
    "        temperature += learning_rate * (temperature * (np.exp(diff)) - temperature)\n",
    "\n",
    "    return temperature\n",
    "\n",
    "temperature = auto_tuning_temperature_heuristic(model, x_valid_new, y_valid_new)\n",
    "\n",
    "# use best temperature when test\n",
    "tuned_test_probs = softmax(np.log(raw_test_probs)/temperature, axis=1)\n",
    "tuned_test_preds = np.argmax(tuned_test_probs, axis=1)\n",
    "print(classification_report(y_pred=tuned_test_preds, y_true=y_test_new))\n",
    "print(confusion_matrix(y_pred=tuned_test_preds, y_true=y_test_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于梯度的温度后调整策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_tuning_temperature(model, x_val, y_val):\n",
    "    ''' 调整温度，使得 model 的预测输出分布 和 y_val 中分布接近\n",
    "    @params model: 模型，需要有方法 predict_proba\n",
    "    '''\n",
    "    label_counter = Counter(y_val)\n",
    "    temperature = torch.tensor([1.0] * len(label_counter), requires_grad=True)\n",
    "    y_prob = torch.from_numpy(model.predict_proba(x_val))\n",
    "    y_true_distrib = torch.tensor([label_counter[key]/len(y_valid) for key in sorted(label_counter.keys())])\n",
    "    \n",
    "    learning_rate = 1e-1\n",
    "    for t in range(10000):\n",
    "        y_prob_tuned = torch.softmax(torch.log(y_prob) / temperature, axis=1)\n",
    "        y_pred_distrib = y_prob_tuned.sum(axis=0) / y_prob_tuned.sum()\n",
    "        loss = torch.sum(torch.square(y_pred_distrib - y_true_distrib))\n",
    "        if t % 1000 == 999:\n",
    "            print(f\"========= iteration: {t} ========= {torch.max(torch.abs(y_pred_distrib - y_true_distrib))}\")\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            temperature -= learning_rate * temperature.grad\n",
    "            # Manually zero the gradients after updating weights\n",
    "            temperature.grad.zero_()\n",
    "\n",
    "    return temperature.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= iteration: 999 ========= 0.004163452038374538\n",
      "========= iteration: 1999 ========= 0.0009165389095672632\n",
      "========= iteration: 2999 ========= 0.00019727509290959344\n",
      "========= iteration: 3999 ========= 4.2736605326595956e-05\n",
      "========= iteration: 4999 ========= 9.985426026454247e-06\n",
      "========= iteration: 5999 ========= 4.04992745470234e-06\n",
      "========= iteration: 6999 ========= 4.04992745470234e-06\n",
      "========= iteration: 7999 ========= 4.04992745470234e-06\n",
      "========= iteration: 8999 ========= 4.04992745470234e-06\n",
      "========= iteration: 9999 ========= 4.04992745470234e-06\n",
      "diff: 0.10674999999999996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.26      0.39       202\n",
      "           1       0.61      0.36      0.45       444\n",
      "           2       0.79      0.83      0.81       430\n",
      "           3       0.57      0.50      0.53       951\n",
      "           4       0.72      0.88      0.79      1973\n",
      "\n",
      "    accuracy                           0.69      4000\n",
      "   macro avg       0.70      0.57      0.60      4000\n",
      "weighted avg       0.69      0.69      0.67      4000\n",
      "\n",
      "[[  53   11   56   13   69]\n",
      " [   0  160    1  135  148]\n",
      " [   5    0  358    9   58]\n",
      " [   1   64   22  471  393]\n",
      " [   8   26   14  193 1732]]\n",
      "diff: 0.12925000000000003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.24      0.37       202\n",
      "           1       0.64      0.28      0.39       444\n",
      "           2       0.87      0.79      0.83       430\n",
      "           3       0.56      0.50      0.53       951\n",
      "           4       0.70      0.89      0.78      1973\n",
      "\n",
      "    accuracy                           0.69      4000\n",
      "   macro avg       0.71      0.54      0.58      4000\n",
      "weighted avg       0.68      0.69      0.66      4000\n",
      "\n",
      "[[  49    7   34   21   91]\n",
      " [   0  126    1  152  165]\n",
      " [   5    0  340   12   73]\n",
      " [   1   46   11  480  413]\n",
      " [   8   19    4  194 1748]]\n",
      "diff: 0.11425000000000002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.25      0.38       192\n",
      "           1       0.55      0.36      0.43       411\n",
      "           2       0.80      0.83      0.82       415\n",
      "           3       0.62      0.50      0.55      1048\n",
      "           4       0.71      0.88      0.79      1934\n",
      "\n",
      "    accuracy                           0.69      4000\n",
      "   macro avg       0.69      0.56      0.59      4000\n",
      "weighted avg       0.68      0.69      0.67      4000\n",
      "\n",
      "[[  48   14   51   11   68]\n",
      " [   0  148    2  122  139]\n",
      " [  11    1  346    8   49]\n",
      " [   0   76   25  519  428]\n",
      " [   5   32    7  183 1707]]\n",
      "diff: 0.1355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.23      0.36       192\n",
      "           1       0.59      0.30      0.40       411\n",
      "           2       0.88      0.80      0.84       415\n",
      "           3       0.60      0.51      0.55      1048\n",
      "           4       0.70      0.89      0.78      1934\n",
      "\n",
      "    accuracy                           0.69      4000\n",
      "   macro avg       0.71      0.55      0.58      4000\n",
      "weighted avg       0.68      0.69      0.67      4000\n",
      "\n",
      "[[  45   13   29   17   88]\n",
      " [   0  123    1  135  152]\n",
      " [  11    0  330   10   64]\n",
      " [   0   52   12  533  451]\n",
      " [   3   21    2  187 1721]]\n"
     ]
    }
   ],
   "source": [
    "auto_tuned_temperature = auto_tuning_temperature(model, x_valid, y_valid)\n",
    "\n",
    "\n",
    "def evaluate_with_temperature(model, x_test, y_test, temperature):\n",
    "    y_prob = model.predict_proba(x_test)\n",
    "    y_prob_tuned = softmax(np.log(y_prob) / temperature, axis=1)\n",
    "    y_pred_tuned = np.argmax(y_prob_tuned, axis=1)\n",
    "\n",
    "    true_label_counter = Counter(y_test)\n",
    "    y_true_distrib = [true_label_counter[key]/len(y_test) for key in sorted(true_label_counter.keys())]\n",
    "    pred_label_counter = Counter(y_pred_tuned)\n",
    "    y_tune_distrib = [pred_label_counter[key]/len(y_test) for key in sorted(pred_label_counter.keys())]\n",
    "\n",
    "    print(\"diff:\", np.max(np.abs(np.array(y_true_distrib) - np.array(y_tune_distrib))))\n",
    "    print(classification_report(y_pred=y_pred_tuned, y_true=y_test))\n",
    "    print(confusion_matrix(y_pred=y_pred_tuned, y_true=y_test))\n",
    "\n",
    "base_temperature = [1.0] * len(Counter(y_valid))\n",
    "evaluate_with_temperature(model, x_valid, y_valid, auto_tuned_temperature)\n",
    "evaluate_with_temperature(model, x_valid, y_valid, base_temperature)\n",
    "\n",
    "base_temperature = [1.0] * len(Counter(y_test))\n",
    "evaluate_with_temperature(model, x_test, y_test, auto_tuned_temperature)\n",
    "evaluate_with_temperature(model, x_test, y_test, base_temperature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
